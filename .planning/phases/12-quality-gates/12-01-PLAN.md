---
phase: 12-quality-gates
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/commands/verify.js
  - src/lib/constants.js
  - src/router.js
  - bin/gsd-tools.test.cjs
  - bin/gsd-tools.cjs
autonomous: true
requirements:
  - VRFY-01
  - VRFY-02
  - VRFY-03

must_haves:
  truths:
    - "verify deliverables runs the project test suite and reports pass/fail"
    - "verify requirements checks each REQUIREMENTS.md item against linked phases and reports unaddressed items"
    - "verify regression compares before/after test results and flags new failures"
  artifacts:
    - path: "src/commands/verify.js"
      provides: "cmdVerifyDeliverables, cmdVerifyRequirements, cmdVerifyRegression"
      contains: "cmdVerifyDeliverables"
    - path: "bin/gsd-tools.test.cjs"
      provides: "verification command tests"
      contains: "describe.*verify deliverables"
  key_links:
    - from: "src/commands/verify.js"
      to: ".planning/REQUIREMENTS.md"
      via: "fs.readFileSync + regex parsing"
      pattern: "REQUIREMENTS\\.md"
    - from: "src/commands/verify.js"
      to: "test suite execution"
      via: "execSync for test commands"
      pattern: "execSync.*test"
---

<objective>
Create three verification commands: test gating (verify deliverables), requirement checking (verify requirements), and regression detection (verify regression).

Purpose: These commands enable automated quality verification after plan execution, catching failures before they propagate.

Output: Three new verify subcommands with test suite integration, requirement tracing, and regression comparison.
</objective>

<execution_context>
@/home/cam/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/cam/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/codebase/CONVENTIONS.md
@.planning/codebase/ARCHITECTURE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Verification commands implementation</name>
  <files>
    src/commands/verify.js
    src/lib/constants.js
    src/router.js
    bin/gsd-tools.cjs
  </files>
  <action>
Add three new verification commands to `src/commands/verify.js`:

**`cmdVerifyDeliverables(cwd, options, raw)`** — Run project test suite and report results (VRFY-01).
- Read `.planning/config.json` for `test_commands` config (maps framework names to commands, e.g., `{"node": "npm test"}`)
- If no test_commands configured, auto-detect: check for `package.json` (npm test), `mix.exs` (mix test), `go.mod` (go test ./...), `pytest.ini`/`setup.py` (pytest)
- Execute the test command via `execSync` with timeout (60s), capture stdout+stderr
- Parse output for pass/fail counts using the existing `cmdTestRun` patterns from `src/commands/features.js`
- Also run `verify artifacts` and `verify key-links` on any plan path provided via `--plan` flag
- Output: `{ test_result: "pass"|"fail", tests_passed, tests_failed, tests_total, artifacts_valid, key_links_valid, verdict: "pass"|"fail" }`

**`cmdVerifyRequirements(cwd, options, raw)`** — Check requirement completion (VRFY-02).
- Read `.planning/REQUIREMENTS.md`, parse requirement IDs and their `[x]`/`[ ]` status
- For each requirement, check the Traceability table to find its phase
- For each mapped phase, check if phase directory has SUMMARY.md files (indicating work was done)
- Cross-reference: requirement is "addressed" if its phase has summaries OR if req is marked `[x]`
- Flag "unaddressed" requirements: mapped to phase but phase has no summaries AND not marked complete
- Output: `{ total, addressed, unaddressed, unaddressed_list: [{id, phase, reason}] }`

**`cmdVerifyRegression(cwd, options, raw)`** — Compare test results before/after (VRFY-03).
- Accept `--before` and `--after` flags pointing to test result JSON files
- If not provided, check `.planning/memory/test-results.json` for stored baseline
- Compare: any test that was passing in "before" but failing in "after" is a regression
- Output: `{ regressions: [{test_name, before: "pass", after: "fail"}], regression_count, verdict: "pass"|"fail" }`
- If no before/after data available, run tests twice (not practical) — instead, store current results as baseline for next comparison

Wire all three commands:
- Add to verify subcommand routing in `src/router.js` (deliverables, requirements, regression)
- Add COMMAND_HELP entries in `src/lib/constants.js`

Run `npm run build`.
  </action>
  <verify>
Run `npm run build` — succeeds.
Run `node bin/gsd-tools.cjs verify deliverables --raw` — returns test result JSON.
Run `node bin/gsd-tools.cjs verify requirements --raw` — returns requirement status.
Run `node bin/gsd-tools.cjs verify --help` — shows all subcommands.
  </verify>
  <done>
Three verification commands exist (deliverables, requirements, regression), produce structured JSON, and are wired into the CLI router with help text.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verification command test suite</name>
  <files>
    bin/gsd-tools.test.cjs
  </files>
  <action>
Add test suites for the new verification commands:

**`describe('verify deliverables')`** — 4 tests:
1. Returns pass when test command succeeds (create a tmp project with passing tests)
2. Returns fail when test command fails
3. Auto-detects test framework from package.json
4. Includes artifacts validation when --plan provided

**`describe('verify requirements')`** — 4 tests:
1. Returns all addressed when requirements are marked [x]
2. Detects unaddressed requirement (phase has no summaries)
3. Returns empty unaddressed list when all phases have summaries
4. Handles missing REQUIREMENTS.md gracefully

**`describe('verify regression')`** — 3 tests:
1. Detects regression when test fails in after but passed in before
2. Returns clean verdict when no regressions
3. Handles missing baseline data gracefully

Run `npm test` — all pass.
  </action>
  <verify>
Run `npm test` — all tests pass (0 failures).
  </verify>
  <done>
11 test cases covering all three verification commands. All pass alongside existing suite.
  </done>
</task>

</tasks>

<verification>
- `npm run build` succeeds
- `npm test` passes all tests
- `verify deliverables` runs tests and reports results
- `verify requirements` checks requirement completion
- `verify regression` compares test results
</verification>

<success_criteria>
- Three verification commands exist and produce structured JSON
- Test gating works: verify deliverables reports pass/fail
- Requirement checking: verify requirements finds unaddressed items
- Regression detection: verify regression compares before/after
- 11+ new tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/12-quality-gates/12-01-SUMMARY.md`
</output>
