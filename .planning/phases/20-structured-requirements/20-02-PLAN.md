---
phase: 20-structured-requirements
plan: 02
type: execute
wave: 2
depends_on:
  - 20-01
files_modified:
  - src/commands/verify.js
  - src/commands/features.js
  - src/router.js
  - bin/gsd-tools.test.cjs
autonomous: true
requirements:
  - SREQ-03
  - SREQ-04
intent:
  outcome_ids:
    - DO-05
  rationale: "Verification checks deliverables against assertions, not just high-level requirements"

must_haves:
  truths:
    - "verify requirements reports per-assertion pass/fail status, not just requirement-level"
    - "Traceability table in REQUIREMENTS.md can include a test-command column"
    - "verify requirements checks that test commands exist and reports coverage percentage"
    - "trace-requirement shows assertion chain: REQ → assertions → plan → verification status"
  artifacts:
    - path: "src/commands/verify.js"
      provides: "Enhanced cmdVerifyRequirements with per-assertion checking"
      contains: "parseAssertionsMd"
    - path: "src/commands/features.js"
      provides: "Enhanced cmdTraceRequirement with assertion chain display"
      contains: "parseAssertionsMd"
  key_links:
    - from: "src/commands/verify.js"
      to: "src/commands/verify.js"
      via: "parseAssertionsMd internal call"
      pattern: "parseAssertionsMd"
    - from: "src/commands/features.js"
      to: "src/commands/verify.js"
      via: "import parseAssertionsMd"
      pattern: "parseAssertionsMd"
---

<objective>
Enhance the verification and traceability commands to work with structured assertions — giving per-assertion granularity instead of requirement-level pass/fail.

Purpose: SREQ-03 requires test-command mapping and coverage reporting. SREQ-04 requires per-assertion pass/fail in verification output. Together these close the loop: requirement → assertion → evidence → verdict.

Output: Enhanced `verify requirements`, enhanced `trace-requirement`, test-command column support in traceability.
</objective>

<execution_context>
@/home/cam/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/cam/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/INTENT.md
@.planning/phases/20-structured-requirements/20-CONTEXT.md
@.planning/phases/20-structured-requirements/20-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enhance verify requirements with per-assertion checking</name>
  <files>
src/commands/verify.js
bin/gsd-tools.test.cjs
  </files>
  <action>
**Enhance `cmdVerifyRequirements(cwd, options, raw)` in `src/commands/verify.js`:**

Current behavior: Parses `- [x]` / `- [ ]` checkboxes in REQUIREMENTS.md, checks traceability table, reports requirement-level addressed/unaddressed.

New behavior (additive — existing output shape preserved, new fields added):

1. **Load ASSERTIONS.md** if it exists (graceful degradation — if absent, fall back to current behavior):
   ```javascript
   const assertionsPath = path.join(cwd, '.planning', 'ASSERTIONS.md');
   const assertionsContent = safeReadFile(assertionsPath);
   const assertions = assertionsContent ? parseAssertionsMd(assertionsContent) : null;
   ```

2. **Parse test-command column in traceability table**: Extend the existing tracePattern regex to capture an optional third column:
   ```
   | SREQ-01 | Phase 20 | Pending | npm test -- --grep "assertions" |
   ```
   Pattern: `/\| (\w+-\d+) \| Phase (\d+)[^|]*\|[^|]*\|([^|]*)\|/g` — captures req ID, phase, and optional test command.
   Store in `traceMap[reqId] = { phase, testCommand }`.

3. **Check test commands exist**: For each requirement with a test command:
   - Extract the base command (first word before space)
   - Check if executable exists: `which <command>` or known commands (npm, node, mix, go)
   - Report: `test_commands: { total, valid, invalid, commands: [{reqId, command, valid}] }`

4. **Per-assertion verification** (when ASSERTIONS.md exists): For each requirement with assertions:
   - **type: file** assertions: Check if referenced files/patterns exist on disk using glob-like matching
   - **type: cli** assertions: Check if the CLI command described in `when` field is a valid gsd-tools command
   - **type: behavior** assertions: Mark as `needs_human` — can't verify statically
   - **type: api** assertions: Mark as `needs_human` — can't verify statically
   - Track per-assertion: `{ assert, priority, status: 'pass'|'fail'|'needs_human', evidence }`

5. **Enhanced output shape** (backward compatible — existing fields preserved):
   ```json
   {
     "total": 5,
     "addressed": 3,
     "unaddressed": 2,
     "unaddressed_list": [...],
     "assertions": {
       "total": 15,
       "verified": 8,
       "failed": 3,
       "needs_human": 4,
       "must_have_pass": 6,
       "must_have_fail": 2,
       "coverage_percent": 53,
       "by_requirement": {
         "SREQ-01": {
           "assertion_count": 3,
           "pass": 2,
           "fail": 1,
           "assertions": [...]
         }
       }
     },
     "test_commands": {
       "total": 3,
       "valid": 2,
       "invalid": 1,
       "coverage_percent": 60,
       "commands": [...]
     }
   }
   ```

6. **Update rawValue**: When assertions present, show assertion stats: `"5 reqs (3 addressed), 15 assertions (8 pass, 3 fail, 4 human)"`. When absent, keep current behavior.

**IMPORTANT:** The hybrid verification mechanism per CONTEXT.md: static pattern matching for file/artifact checks (automated), agent evaluation for behavioral assertions (needs_human). Failed must-have assertions should include a `gap_description` field that can feed into `--gaps` planning workflow.

**Add 6-8 tests** covering:
- Backward compatibility: verify requirements works identically when no ASSERTIONS.md exists
- Per-assertion pass/fail with file type assertions (create temp files)
- Test-command column parsing from traceability table
- Coverage percentage calculation
- Must-have vs nice-to-have filtering in output
  </action>
  <verify>
```bash
npm run build && npm test
```
All tests pass. Build succeeds. Run `node bin/gsd-tools.cjs verify requirements --raw` against this project to confirm backward compatibility.
  </verify>
  <done>
- verify requirements includes per-assertion status when ASSERTIONS.md exists
- Test-command column parsed from traceability table with validity checking
- Coverage percentage reported for both assertions and test commands
- Failed must-have assertions include gap_description for --gaps workflow
- Backward compatible: projects without ASSERTIONS.md see identical behavior
  </done>
</task>

<task type="auto">
  <name>Task 2: Enhance trace-requirement with assertion chain</name>
  <files>
src/commands/features.js
src/router.js
bin/gsd-tools.test.cjs
  </files>
  <action>
**Enhance `cmdTraceRequirement(cwd, reqId, raw)` in `src/commands/features.js`:**

Current behavior: Shows requirement → phase → plans → files → status. Good but no assertion visibility.

New behavior (additive):

1. **Import parseAssertionsMd** from verify.js:
   ```javascript
   const { parseAssertionsMd } = require('./verify');
   ```

2. **Load assertions for the traced requirement**:
   ```javascript
   const assertionsContent = safeReadFile(path.join(cwd, '.planning', 'ASSERTIONS.md'));
   if (assertionsContent) {
     const allAssertions = parseAssertionsMd(assertionsContent);
     const reqAssertions = allAssertions[reqUpper];
     if (reqAssertions) {
       trace.assertions = reqAssertions.assertions.map(a => ({
         assert: a.assert,
         priority: a.priority,
         type: a.type || null,
       }));
       trace.assertion_count = reqAssertions.assertions.length;
       trace.must_have_count = reqAssertions.assertions.filter(a => a.priority === 'must-have').length;
     }
   }
   ```

3. **Add assertion status to trace output**: For each assertion, cross-reference with plan's must_haves.truths:
   - If assertion text appears in any plan's must_haves.truths → `planned: true`
   - If plan has SUMMARY → `implemented: true`  
   - If neither → `gap: true`

4. **Enhanced trace output** (backward compatible):
   ```json
   {
     "requirement": "SREQ-01",
     "phase": "20",
     "plans": [...],
     "files": [...],
     "status": "implemented",
     "assertions": [
       {
         "assert": "Each requirement has 2-5 testable assertions",
         "priority": "must-have",
         "type": "file",
         "planned": true,
         "implemented": true,
         "gap": false
       }
     ],
     "assertion_count": 3,
     "must_have_count": 2,
     "chain": "SREQ-01 → 3 assertions (2 must-have) → Plan 20-01 → IMPLEMENTED"
   }
   ```

5. **Add `chain` field**: Human-readable status chain matching the format from CONTEXT.md:
   `"SREQ-01 → ASSERT-01a ✓, ASSERT-01b ✗ → Plan 01 Task 1 → VERIFICATION: partial"`

**Add 4-6 tests** covering:
- trace-requirement with assertions present shows assertion data
- trace-requirement without ASSERTIONS.md still works (backward compat)
- Chain format is correct
- Assertion planned/implemented/gap status cross-referencing
  </action>
  <verify>
```bash
npm run build && npm test
```
All tests pass. `node bin/gsd-tools.cjs trace-requirement SREQ-01 --raw` shows assertion chain when ASSERTIONS.md exists.
  </verify>
  <done>
- trace-requirement shows assertions, their priorities, and planned/implemented status
- Chain field provides human-readable status chain per CONTEXT.md format
- Backward compatible: no ASSERTIONS.md → existing output unchanged
- 4+ new tests pass
  </done>
</task>

</tasks>

<verification>
1. `node bin/gsd-tools.cjs verify requirements --raw` shows per-assertion pass/fail when ASSERTIONS.md exists
2. `node bin/gsd-tools.cjs verify requirements --raw` works identically to before when no ASSERTIONS.md
3. `node bin/gsd-tools.cjs trace-requirement SREQ-01 --raw` shows assertion chain with status
4. `npm test` passes all existing + new tests
5. `npm run build` stays under 525KB
</verification>

<success_criteria>
- Verification reports per-assertion granularity (not just requirement-level)
- Traceability includes full assertion chain visibility
- Coverage percentage captures both assertion coverage and test-command coverage
- Failed must-have assertions generate gap descriptions for --gaps workflow
</success_criteria>

<output>
After completion, create `.planning/phases/20-structured-requirements/20-02-SUMMARY.md`
</output>
